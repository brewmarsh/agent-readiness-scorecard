import os

def generate_markdown_report(results):
    """Generates a Markdown report from the analysis results based on the new Agent Readiness spec."""
    final_score = results["final_score"]
    file_results = results["file_results"]
    profile = results["profile"]
    agent_name = results["agent"]

    # --- 1. Executive Summary ---
    summary = f"# Agent Scorecard Report\n\n"
    summary += f"**Target Agent Profile:** {agent_name.upper()}\n"
    summary += f"**Description:** {profile['description']}\n\n"
    summary += f"## Final Score: {final_score:.1f}/100\n\n"

    if final_score >= 70:
        summary += "‚úÖ **Status: PASSED** - This codebase is Agent-Ready.\n\n"
    else:
        summary += "‚ùå **Status: FAILED** - This codebase needs improvement for AI Agents.\n\n"

    if results["missing_docs"]:
        summary += "### ‚ö†Ô∏è Missing Critical Documentation\n"
        for doc in results["missing_docs"]:
            summary += f"- `{doc}` (-15 pts)\n"
        summary += "\n"

    # --- 2. Top Refactoring Targets (ACL) ---
    targets = "## üéØ Top Refactoring Targets (Agent Cognitive Load)\n\n"
    targets += "ACL = Complexity + (Lines of Code / 20). Target: ACL <= 10.\n\n"

    all_functions = []
    for f_res in file_results:
        for m in f_res.get("function_metrics", []):
            all_functions.append({**m, "file": f_res["file"]})

    # Sort by ACL descending
    top_acl = sorted(all_functions, key=lambda x: x['acl'], reverse=True)[:10]

    if top_acl:
        targets += "| Function | File | ACL | Status |\n"
        targets += "|----------|------|-----|--------|\n"
        for fn in top_acl:
            if fn['acl'] > 10:
                status = "üî¥ Red" if fn['acl'] > 20 else "üü° Yellow"
                targets += f"| `{fn['name']}` | `{fn['file']}` | {fn['acl']:.1f} | {status} |\n"
        targets += "\n"
    else:
        targets += "‚úÖ No functions with high cognitive load found.\n\n"

    # --- 3. Type Safety Index ---
    types_section = "## üõ°Ô∏è Type Safety Index\n\n"
    types_section += "Target: >90% of functions must have explicit type signatures.\n\n"

    types_section += "| File | Type Safety Index | Status |\n"
    types_section += "| :--- | :---------------: | :----- |\n"
    for res in file_results:
        status = "‚úÖ" if res["type_coverage"] >= 90 else "‚ùå"
        types_section += f"| {res['file']} | {res['type_coverage']:.0f}% | {status} |\n"
    types_section += "\n"

    # --- 4. Agent Prompts ---
    prompts = "## ü§ñ Agent Prompts for Remediation\n\n"

    problematic_files = [f for f in file_results if f["score"] < 90]

    for f_res in problematic_files:
        file_path = f_res["file"]
        file_issues = []

        red_functions = [m for m in f_res["function_metrics"] if m["acl"] > 20]
        yellow_functions = [m for m in f_res["function_metrics"] if 10 < m["acl"] <= 20]

        if red_functions:
            fn_names = ", ".join([f"`{m['name']}`" for m in red_functions])
            file_issues.append(f"- **Critical ACL**: Functions {fn_names} have Red ACL (>20). Prompt: 'Refactor functions in `{file_path}` with high cognitive load to bring ACL below 10. Split complex logic and reduce function length.'")

        if f_res["type_coverage"] < 90:
             file_issues.append(f"- **Type Safety**: Coverage is {f_res['type_coverage']:.0f}%. Prompt: 'Add explicit type signatures to all functions in `{file_path}` to meet the 90% Type Safety Index requirement.'")

        if file_issues:
            prompts += f"### File: `{file_path}`\n"
            prompts += "\n".join(file_issues) + "\n\n"

    if not problematic_files:
        prompts += "‚úÖ Codebase is optimized for AI Agents. No immediate prompts needed.\n\n"

    # --- 5. File Analysis Table ---
    table = "### üìÇ Full File Analysis\n\n"
    table += "| File | Score | Issues |\n"
    table += "| :--- | :---: | :--- |\n"
    for res in file_results:
        status = "‚úÖ" if res["score"] >= 70 else "‚ùå"
        table += f"| {res['file']} | {res['score']} {status} | {res['issues']} |\n"

    return summary + targets + types_section + prompts + "\n" + table + "\n---\n*Generated by Agent-Scorecard*"
