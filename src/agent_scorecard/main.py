import os
import sys
import click
from importlib.metadata import version, PackageNotFoundError
from rich.console import Console
from rich.table import Table
from rich.panel import Panel

# Import common modules
from . import analyzer, report

# Use the Modular Refactor imports
from .constants import PROFILES
from .checks import scan_project_docs
from .fix import apply_fixes
from .scoring import score_file, generate_badge

console = Console()

def perform_analysis(path, agent_name):
    """
    Core analysis logic that returns data for presentation.
    Separating this makes integration testing easier.
    """
    if agent_name not in PROFILES:
        agent_name = "generic"
    profile = PROFILES[agent_name]

    # 1. Project Level Check
    project_score = 100
    missing_docs = []
    if os.path.isdir(path):
        missing_docs = scan_project_docs(path, profile["required_files"])
        penalty = len(missing_docs) * 15
        project_score = max(0, 100 - penalty)

    # 2. Gather Files
    py_files = []
    if os.path.isfile(path) and path.endswith(".py"):
        py_files = [path]
    elif os.path.isdir(path):
        for root, _, files in os.walk(path):
            for file in files:
                if file.endswith(".py"):
                    py_files.append(os.path.join(root, file))

    # 3. File Level Check
    file_results = []
    for filepath in py_files:
        score, issues = score_file(filepath, profile)
        rel_path = os.path.relpath(filepath, start=path if os.path.isdir(path) else os.path.dirname(path))
        file_results.append({
            "file": rel_path,
            "score": score,
            "issues": issues
        })

    # 4. Aggregation
    avg_file_score = sum(f["score"] for f in file_results) / len(file_results) if file_results else 0
    final_score = (avg_file_score * 0.8) + (project_score * 0.2)

    return {
        "agent": agent_name,
        "profile": profile,
        "final_score": final_score,
        "project_score": project_score,
        "missing_docs": missing_docs,
        "file_results": file_results
    }


def generate_markdown_report(results):
    """Formats analysis results into a Markdown string."""
    profile = results["profile"]
    md = f"# Agent Scorecard Report\n\n"
    md += f"**Target Agent Profile:** {results['agent'].upper()}\n"
    md += f"**Description:** {profile['description']}\n\n"
    md += f"## Final Score: {results['final_score']:.1f}/100\n\n"

    if results["final_score"] >= 70:
        md += "âœ… **Status: PASSED** - This codebase is Agent-Ready.\n\n"
    else:
        md += "âŒ **Status: FAILED** - This codebase needs improvement for AI Agents.\n\n"

    if results["missing_docs"]:
        md += "### âš  Missing Critical Documentation\n"
        for doc in results["missing_docs"]:
            md += f"- `{doc}` (-15 pts)\n"
        md += "\n"

    md += "### ðŸ“‚ File Analysis\n\n"
    md += "| File | Score | Issues |\n"
    md += "| :--- | :---: | :--- |\n"
    for res in results["file_results"]:
        status = "âœ…" if res["score"] >= 70 else "âŒ"
        md += f"| {res['file']} | {res['score']} {status} | {res['issues']} |\n"

    md += "\n---\n*Generated by Agent-Scorecard*"
    return md


try:
    __version__ = version("agent-scorecard")
except PackageNotFoundError:
    __version__ = "0.0.0"

@click.group()
@click.version_option(version=__version__)
def cli():
    """Main entry point for the agent-scorecard CLI."""
    pass


@cli.command(name="score")
@click.argument("path", default=".", type=click.Path(exists=True))
@click.option("--agent", default="generic", help="Profile to use: generic, jules, copilot.")
@click.option("--fix", is_flag=True, help="Automatically fix common issues.")
@click.option("--badge", is_flag=True, help="Generate an SVG badge for the score.")
@click.option("--report", "report_path", type=click.Path(), help="Save the report to a Markdown file.")
def score(path: str, agent: str, fix: bool, badge: bool, report_path: str) -> None:
    """Scores a codebase based on AI-agent compatibility."""

    if agent not in PROFILES:
        console.print(f"[bold red]Unknown agent profile: {agent}. using generic.[/bold red]")
        agent = "generic"
    
    profile = PROFILES[agent]

    # 1. Apply Fixes first if requested
    if fix:
        console.print(Panel(f"[bold cyan]Applying Fixes[/bold cyan]\nProfile: {agent.upper()}", expand=False))
        apply_fixes(path, profile)
        console.print("")

    # 2. Run Analysis
    results = perform_analysis(path, agent)

    console.print(Panel(f"[bold cyan]Running Agent Scorecard[/bold cyan]\nProfile: {agent.upper()}\n{profile['description']}", expand=False))

    if results["missing_docs"]:
        penalty = len(results["missing_docs"]) * 15
        console.print(f"\n[bold yellow]âš  Missing Critical Agent Docs:[/bold yellow] {', '.join(results['missing_docs'])} (-{penalty} pts)")

    table = Table(title="File Analysis")
    table.add_column("File", style="cyan")
    table.add_column("Score", justify="right")
    table.add_column("Issues", style="magenta")

    for res in results["file_results"]:
        status_color = "green" if res["score"] >= 70 else "red"
        table.add_row(res["file"], f"[{status_color}]{res['score']}[/{status_color}]", res["issues"])

    console.print(table)
    console.print(f"\n[bold]Final Agent Score: {results['final_score']:.1f}/100[/bold]")

    if badge:
        output_path = "agent_score.svg"
        svg_content = generate_badge(results["final_score"])
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(svg_content)
        console.print(f"[bold green][Generated][/bold green] Badge saved to ./{output_path}")
        console.print(f"\nMarkdown Snippet:\n[![Agent Score]({output_path})](./{output_path})")

    # MERGE RESOLUTION: Use the helper function instead of manual string building
    if report_path:
        report_content = generate_markdown_report(results)
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)
        console.print(f"\n[bold green]Report saved to {report_path}[/bold green]")

    if results["final_score"] < 70:
        console.print("[bold red]FAILED: Not Agent-Ready[/bold red]")
        sys.exit(1)
    else:
        console.print("[bold green]PASSED: Agent-Ready[/bold green]")


@cli.command(name="advise")
@click.argument("path", default=".", type=click.Path(exists=True))
@click.option("--output", "-o", "output_file", type=click.Path(), help="Save the report to a Markdown file.")
@click.option("--agent", default="generic", help="Profile to use.")
def advise(path, output_file, agent):
    """Generates a Markdown report with actionable advice."""
    
    console.print(Panel("[bold cyan]Running Advisor Mode[/bold cyan]", expand=False))
    
    # Re-use the perform_analysis logic!
    results = perform_analysis(path, agent)
    report_content = generate_markdown_report(results)

    if output_file:
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(report_content)
        console.print(f"\n[bold green]Report saved to {output_file}[/bold green]")
    else:
        console.print("\n" + report_content)

if __name__ == "__main__":
    cli()